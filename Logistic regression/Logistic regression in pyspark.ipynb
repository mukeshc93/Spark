{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression in Apache pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Apache Spark is becoming a fast and efficient way to analyse large amounts of data. In our project we are using the python api of spark to analyse the HR analytics data obtained from Kaggle. This dataset has more than a million observations and the objective of our project would be to predict whether an employee will leave the company or not. It is a binary classification problem and independent features include satifaction levels, last evaluation, number of projects completed, average monthly hours dedicated, time spent, last appraisals etc. \n",
    "\n",
    "In the below report the logistic regression model will be used to predict whether the employee will leave or not. The following report and codes will help in better understanding the steps and reasoning behind the algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "These days companies find it difficult to find how long can an employee stay in their company. Any sudden leave or resignation can cause losses to the companies, the losses may have a less monetary impact but the impact on overall productivity of the company gets hampered as they lose in terms of time. Also finding alternate and reliable human resources can sometimes be a problem. If we are able to find out how likely is an employee about to leave then we can take necessary steps so as to minimize the losses. These steps will be decided by the company, but some of the common examples can be trying to retain the employee or start looking for new ones in advance.\n",
    "\n",
    "This advance knowledge can be of great help for the company. Since we have data being collected all around, we must find ways to best utilize it. Therefore we collected a sample of past behaviour of employees and try to predict the likelihood of leaving. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "The design of our model can be broken down into following steps:\n",
    "\n",
    "1. Loading the required libraries.\n",
    "2. Since we are using spark, we will have to initialize and create a spark session which will be responsible for managing                                                                     and maintaining the resources to be allocated for the processes.\n",
    "3. Loading the dataset and doing the required pre-processing and data conversions so that the data can be used in the model building process.\n",
    "4. Splitting into train and test and building the model on train.\n",
    "5. Model evaluations based on suitable metrics and finding how the model performs on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###                        Step 1: Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SQLContext#For loading the csv files as dataframes\n",
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.ml.tuning import TrainValidationSplit#For doing train test split\n",
    "from pyspark.ml.classification import LogisticRegression#model builder function\n",
    "from pyspark.sql import SparkSession#to create spark session\n",
    "from pyspark.ml import Pipeline#Pipeline for creating a flow of processes to be done on data\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler#Data converson functions\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator#Model evaluator function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initializing the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"LR\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Loading data and pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('/home/meaww/Downloads/HR_comma_sep.csv')\n",
    "cols=df.columns#getting column names of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(satisfaction_level=0.38, last_evaluation=0.53, number_project=2, average_montly_hours=157, time_spend_company=3, Work_accident=0, left=1, promotion_last_5years=0, sales=u'sales', salary=u'low'),\n",
       " Row(satisfaction_level=0.8, last_evaluation=0.86, number_project=5, average_montly_hours=262, time_spend_company=6, Work_accident=0, left=1, promotion_last_5years=0, sales=u'sales', salary=u'medium'),\n",
       " Row(satisfaction_level=0.11, last_evaluation=0.88, number_project=7, average_montly_hours=272, time_spend_company=4, Work_accident=0, left=1, promotion_last_5years=0, sales=u'sales', salary=u'medium'),\n",
       " Row(satisfaction_level=0.72, last_evaluation=0.87, number_project=5, average_montly_hours=223, time_spend_company=5, Work_accident=0, left=1, promotion_last_5years=0, sales=u'sales', salary=u'low'),\n",
       " Row(satisfaction_level=0.37, last_evaluation=0.52, number_project=2, average_montly_hours=159, time_spend_company=3, Work_accident=0, left=1, promotion_last_5years=0, sales=u'sales', salary=u'low')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)#Viewing the first 5 rows of the data get some idea about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['satisfaction_level',\n",
       " 'last_evaluation',\n",
       " 'number_project',\n",
       " 'average_montly_hours',\n",
       " 'time_spend_company',\n",
       " 'Work_accident',\n",
       " 'left',\n",
       " 'promotion_last_5years',\n",
       " 'sales',\n",
       " 'salary']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols#Viewing the column names of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Identifying the categorical variables so that they can be encoded as numeric to be integrated in the model\n",
    "catcols=[\"sales\",\"salary\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "In the below steps we will encode the categorical columns into numeric. We will create extra columns which will represent the encoded features. \n",
    "\n",
    "We will also define a flow so that the data can be inserted into a pipeline and the necessary pre-processing steps will be performed. The final result would be vector of features representing the indenpendent variables and the label vector which will be the output variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stages=[]\n",
    "for c in catcols:\n",
    "    strIndexer=StringIndexer(inputCol=c, outputCol=c+\"Index\")\n",
    "    encoder=OneHotEncoder(inputCol=c+\"Index\", outputCol=c+\"classVec\")\n",
    "    stages = stages + [strIndexer,encoder]\n",
    "    \n",
    "label_idx=StringIndexer(inputCol=\"left\",outputCol=\"label\")\n",
    "stages = stages+[label_idx]\n",
    "    \n",
    "numcols=[\"satisfaction_level\",\"last_evaluation\",\"number_project\",\"average_montly_hours\",\"time_spend_company\",\n",
    "         \"Work_accident\",\"promotion_last_5years\"]\n",
    "\n",
    "assem_ip=map(lambda c: c+ \"classVec\", catcols) + numcols\n",
    "assembler=VectorAssembler(inputCols=assem_ip,outputCol=\"features\")\n",
    "stages=stages+[assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Doing all the process of stages in a pipeline\n",
    "pipeline=Pipeline(stages=stages)\n",
    "\n",
    "pl_ml=pipeline.fit(df)\n",
    "df=pl_ml.transform(df)\n",
    "\n",
    "sel_cols=[\"label\",\"features\"]+cols\n",
    "\n",
    "df=df.select(sel_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=1.0, features=SparseVector(18, {0: 1.0, 9: 1.0, 11: 0.38, 12: 0.53, 13: 2.0, 14: 157.0, 15: 3.0}), satisfaction_level=0.38, last_evaluation=0.53, number_project=2, average_montly_hours=157, time_spend_company=3, Work_accident=0, left=1, promotion_last_5years=0, sales=u'sales', salary=u'low')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()#Viewing the first row of the transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Splitting into train and test and model building process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.75, 0.25], seed=141)#Splitting into train and test\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.3)#buiding a linear regression model\n",
    "# Fit the model on the train data\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Model evaluation process"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this step we will test and evaluate the model. The best practice is to test its performance on unseen data. Since we have the test data which is completely unknown to the model, we will test the performance on this data. We will use a binary classification evaluator which will define the performance of the model. The performance of the logistic regression model can be derived by the ROC curve. The value of this metric ranges from 0.5 to 1.0 and high values indicate that our model is doing a good job in dircriminating between the two categories which comprise our label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8052813342286241"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(test)#Predicting on test data\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")#Calling the evaluator function\n",
    "evaluator.evaluate(predictions)#Getting models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0241288152874,0.0311345364118,0.0515342912541,-0.0259221737696,0.000719485546598,-0.0299886908688,-0.194978042699,0.0599593637189,0.125851517991,0.204817567347,-0.0394367734893,-1.32491947181,0.0144600802804,-0.0167936562558,0.00108419081115,0.0783239547735,-0.386446828376,-0.369147881284]\n",
      "Intercept: -0.880250209332\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges Faced:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "These were some of the the challenges which were faced during this project\n",
    "\n",
    "1. Spark installation: Getting spark up and running can sometimes have issues, so to get a hassle free installation we installed it on a ubuntu machine.\n",
    "2. Defining the problem: Since this was an open problem we had to find a problem and come up with its solution.\n",
    "3. Getting data into the format which spark ML libraries can process: Spark requires data to be provided in a specific format and hence finding the correct format and assembling the data in vector format was also one of the challenges.\n",
    "4. Model tuning and evaluation: Tuning the model to get good results was also one of the challenge. We had to increase the iterations count to get good results. Als evaluating it on test data and getting good results was also one of the challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We were successfully able to get good accuracy in predicting whether an employee will leave or not. The objective of the project was served and this model will be very helpful for the HR departments of the companies to get a productivity boost of the entire organization. They would be well aware about the employees behaviour and necesaary steps would have been taken in advance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
